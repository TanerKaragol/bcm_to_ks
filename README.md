# Nvidia BCM to Kickstart Migration Guide

If you need to leave BCM for any reason (license policy, price, etc.) or simply want to add new nodes without buying additional licenses, this guide explains how to migrate using **Kickstart**.

## Environment & Versions
* **Virtualization:** KVM/QEMU
* **OS Versions:** Rocky 9.2, Bright 9.2
* **Existing Services:** We dont use new services for Kickstart. We will use BCM's PXE, DHCP, DNS, and TFTP to to keep envirement simple (KISS prinsible).

---

## 1. Key Challenges & Solutions

* **Manual Registration:** Since BCM won't allow new nodes without a license, we handle PXE, DHCP, DNS, TFTP, and Slurm entries manually.
* **UUID Management:** Disk UUIDs are updated dynamically using `sed` within the Kickstart post-install script.
* **Boot Partitioning:** If `/boot` partition exists, add `insmod xfs` or `insmod ext` in `grub.cfg`.
* **Boot Partitioning:** If `/boot` partition exists, the path in BLS (.conf) files should not include `/boot/` (e.g., `/vmlinuz` instead of `/boot/vmlinuz`). We handle this with `sed`.
* **Master Image:** We are still using BCM master image. You dont have to create seperate image for Kicskstart.

---

## 2. Infrastructure Setup

### PXE Boot Preparation
Mount the Rocky ISO and copy the necessary boot files:

```bash
mkdir /mnt/iso
mount -o loop Rocky-9.2-x86_64-boot.iso /mnt/iso
cp /mnt/iso/images/pxeboot/vmlinuz /tftpboot/images/vmlinuz-rocky92
cp /mnt/iso/images/pxeboot/initrd.img /tftpboot/images/initrd-rocky92.img
```

**Exporting BCM Images**

Export the BCM image from the head node via NFS:
```bash
# Add to /etc/exports:
/cm/images/<image_path> 192.168.122.0/24(ro,sync,no_subtree_check,no_root_squash)
exportfs -r
systemctl restart nfs-server
```
---
## 3. Kickstart Configuration

Copy bcm.ks here /var/www/html/ks/bcm.ks. Then modify it corresponding to your needs. For example disk device name, password.
For Password Encryption: To generate the hashed password for the Kickstart file:
```bash
openssl passwd -6
```
You will use hashed password output in kiskstart file.

---
## 4. Manual Node Registration
**PXE Configuration**

Copy a BCM category template file with a new name and modify it for manual use:
```bash
# Location: /tftpboot/pxelinux.cfg/default.default
cp /tftpboot/pxelinux.cfg/category.default /root/default
```
Add fallowing lines  at the begining of the "default" file and close "MENU DEFAULT" under "LABEL linux" section by adding "#" at the beginning of line.
```bash
DEFAULT NEW-NODE-INSTALL
PROMPT 1
TIMEOUT 30

LABEL NEW-NODE-INSTALL
  MENU LABEL ^NEW          - New Node Install
  KERNEL images/vmlinuz-rocky92
  APPEND initrd=images/initrd-rocky92.img inst.ks=http://192.168.122.254/ks/bcm.ks inst.text crashkernel=0
  MENU DEFAULT
...
# Closed line
# MENU DEFAULT
```
Copy modified "default" file under tftp folder and make it unchangable (by BCM).
```bash
cp /root/default /tftpboot/pxelinux.cfg/default && chattr +i /tftpboot/pxelinux.cfg/default
```
**DHCP & DNS (Manual)**

Add unlicensed nodes to BCM-excluded config files:

DHCP (/etc/dhcpd.include.conf):
```bash
host cn11 {
  hardware ethernet 52:54:00:20:04:dd;
  fixed-address 192.168.122.21;
  next-server <PXE_Server_IP>;
  filename "x86_64/bios/lpxelinux.0";  #BIOS Legacy Mode
#  filename "x86_64/efi64/syslinux.efi"; #UEFI Mode
}
```
DNS (/var/named/eth.cluster.zone.include):
```bash
cn11    IN A 192.168.122.21
```
---
## 5. Slurm Configuration

Add your new nodes to Slurm manually in /cm/shared/apps/slurm/var/etc/slurm/slurm.conf after the BCM "autogenerated" section:
```bash
# END AUTOGENERATED SECTION
NodeName=cn[11-20] Procs=1 State=UNKNOWN
PartitionName="newNodes" Nodes=cn[01-20]
```
---
## 6. End
Restart all services that we have modifed: DHCP, DNS, TFTP, SLURM.

Boot the new node with PXE boot. Select "New Node Install" from the menu.

---
# Notes & Known Issues

* Slurmd Service: May require a one-time manual start after the first boot.
* Optimization: If your dont want to create separete partition for /boot, you can simplify the Kickstart and skip the sed path replacements.
* SLURM: If SLURM is managed by BMC, you cant add new nodes to exisiting partitions. 
